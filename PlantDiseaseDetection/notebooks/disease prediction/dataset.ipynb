{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["L7cQyf-1nwEm"],"toc_visible":true,"authorship_tag":"ABX9TyNz78kkfoYHk8p9WQglmims"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## TensorFlow DataSet 만들기\n","- 기존에 생성한 학습 데이터 폴더를 활용하여 TensorFlow DataSet을 생성한다."],"metadata":{"id":"EIm7cH-En1y7"}},{"cell_type":"code","source":["# 데이터 정보\n","cfg = {\n","    'train_size': (512, 512),  # 인풋 이미지 크기\n","    'dict_label': {\n","        '02': {'00': 0, '03': 1, '04': 2},  # 고추마일드, 고추점무늬\n","        '05': {'00': 3, '09': 4, '10': 5},  # 상추균핵병, 상추노균병\n","        '11': {'00': 6, '18': 7, '19': 8},  # 토마토황화잎, 토마토잎곰팡이\n","    }\n","}\n","num_classes = 3 + 3 + 3"],"metadata":{"id":"OcbyMXU6pXlu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 데이터 정제 (Data Cleaning)\n","- 라벨링이 잘못된 데이터를 삭제\n","  - conf['dict_label']에 없는 라벨을 가진 데이터는 삭제한다."],"metadata":{"id":"L7cQyf-1nwEm"}},{"cell_type":"code","source":["# 데이터 로드\n","\n","all_data = []\n","root_path = f\"/content/{Dataset_name}/\"\n","\n","for root, dirs, files in os.walk(root_path+\"train\"):\n","        for file in files:\n","            all_data.append(os.path.join(root, file))\n","\n","for root, dirs, files in os.walk(root_path+\"valid\"):\n","        for file in files:\n","            all_data.append(os.path.join(root, file))\n","\n","\n","print(len(all_data))"],"metadata":{"id":"ckPYxVmco15d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","def delete_file(file_path):\n","    try:\n","        # 파일이 존재하는지 확인\n","        if os.path.isfile(file_path):\n","            os.remove(file_path)\n","            print(f\"{file_path} 파일이 성공적으로 삭제되었습니다.\")\n","        else:\n","            print(f\"{file_path} 파일을 찾을 수 없습니다.\")\n","    except Exception as e:\n","        print(f\"{file_path} 파일을 삭제하는 중 오류가 발생했습니다: {e}\")"],"metadata":{"id":"6Z8RIAg6ozSm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 유효한 질병 코드 리스트\n","valid_disease_codes = ['00', '03', '04', '09', '10', '18', '19']\n","valid_area_codes = ['00', '01', '02', '03', '04', '05', '06', '07']\n","valid_type_codes = ['02', '05', '11']\n","\n","error_data_num = 0\n","error_data_paths = []\n","\n","for path in all_data:\n","    basename = os.path.basename(path)\n","    disease = get_plant_disease_code(basename)\n","    type_ = get_plant_type_code(basename)\n","    area = get_plant_area_code(basename)\n","    if not (disease in valid_disease_codes and \\\n","            type_ in valid_type_codes and area in valid_area_codes):\n","        error_data_num += 1\n","        error_data_paths.append(path)\n","\n","# 데이터 삭제\n","for path in error_data_paths:\n","    delete_file(path)\n","    all_data.remove(path)\n","\n","# 삭제된 파일 수 출력\n","print(\"삭제된 데이터 수:\", error_data_num)"],"metadata":{"id":"TJwYVKgNpIak"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 현재 데이터 요약\n","import os\n","\n","# 중복 없이 각 코드들을 저장할 집합\n","area_code = set()\n","type_code = set()\n","\n","# 질병 코드 빈도를 저장할 딕셔너리 초기화\n","disease_code = {}\n","\n","# 학습 데이터에서 질병 코드 빈도 계산\n","for path in all_data:\n","    basename = os.path.basename(path)\n","\n","    # 지역 코드와 타입 코드는 집합에 추가\n","    area_code.add(get_plant_area_code(basename))\n","    type_code.add(get_plant_type_code(basename))\n","\n","    # 질병 코드는 딕셔너리로 빈도 계산\n","    disease = get_plant_disease_code(basename)\n","    if disease not in disease_code:\n","        disease_code[disease] = 0\n","    disease_code[disease] += 0.5\n","\n","# 코드 출력\n","print(\"Type Code:\", type_code)\n","print(\"Area Code:\", area_code)\n","print(\"Disease Code Frequency:\", disease_code)\n"],"metadata":{"id":"8QsH2CjfpRpK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 데이터셋 구축 (Building Dataset)\n","- TensorFlow에서 제공하는 tf.data.Dataset API를 사용하여 데이터셋을 구축하고 관리"],"metadata":{"id":"diFdD2NupUHX"}},{"cell_type":"markdown","source":["#### util 함수\n","- 특정 폴더안의 해당 확장자를 가진 파일들을 읽어오기\n","- 이미지와 라벨이 서로 대응되는지 검수\n","- 파일 이름에서 식물의 특정 코드를(부위, 종류,질병) 추출하는 함수"],"metadata":{"id":"6YnT8TCwpvpD"}},{"cell_type":"code","source":["def list_files_in_folder(folder_path, extensions=None):\n","    file_paths = []\n","    for root, dirs, files in os.walk(folder_path):\n","        for file in files:\n","            if extensions:\n","                if any(file.lower().endswith(ext) for ext in extensions):\n","                    file_paths.append(os.path.join(root, file))\n","            else:\n","                file_paths.append(os.path.join(root, file))\n","    return sorted(file_paths)\n","\n","def is_same_filename(file1, file2):\n","  # 파일 확장자를 제거한 파일 이름을 가져옴\n","    base1 = os.path.basename(file1).split('.')[0]\n","    base2 = os.path.basename(file2).split('.')[0]\n","\n","    # 확장자를 제거한 파일 이름이 같은지 비교\n","    return base1 == base2"],"metadata":{"id":"I73ZXwK_piVf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","# 식물 종류 코드 반환 함수\n","def get_plant_type_code(filename):\n","    # '_'를 기준으로 분할하고 5번째 요소 식물 종류 코드 반환 -> Class name 설정을 위해\n","    token = filename.split('_')\n","    if len(token) >= 7:\n","        return token[4]\n","    else:\n","        return False\n","\n","\n","# 식물 부위 코드 반환 함수\n","def get_plant_area_code(filename):\n","    # '_'를 기준으로 분할하고 6번째 요소 식물 부위 코드 반환\n","    token = filename.split('_')\n","    if len(token) >= 7:\n","        return token[5]\n","    else:\n","        return False\n","\n","# 식물 질병 코드 반환 함수\n","def get_plant_disease_code(filename):\n","    # '_'를 기준으로 분할하고 3번째 요소 식물 질병 코드 반환\n","    token = filename.split('_')\n","    if len(token) >= 7:\n","        return token[3]\n","    else:\n","        return False"],"metadata":{"id":"JVEmFIkCTWov"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### dataset 관련 함수\n","- 이미지와 라벨을 로드 및 전처리하는 함수\n","- 주어진 이미지 파일 경로와 라벨 파일 경로를 사용하여 데이터셋을 생성하는 함수\n","- albumentations 객체\n","  - 데이터 증강 및 정규화 라이브러리"],"metadata":{"id":"wv7TQ9qLQKfV"}},{"cell_type":"code","source":["import json\n","import tensorflow as tf\n","\n","# 이미지와 라벨을 로드 및 전처리하는 함수\n","def load_image_and_label(image_path, label_path):\n","\n","    # 이미지 로드 및 RGB 변환\n","    image = tf.io.read_file(image_path)\n","    image = tf.image.decode_jpeg(image, channels=3)\n","    # 이미지를 원하는 크기로 조정\n","    image = tf.image.resize(image, cfg['train_size'])  # cfg['train_size'] = (512, 512)\n","\n","    # 이미지 정규화\n","    augmented_image = test_aug(image=image.numpy())['image']\n","\n","    ### 파일 이름으로 라벨링\n","    label_path_name = label_path.numpy().decode(\"utf-8\")\n","\n","    # 파일 이름에서 라벨 추출\n","    basename = os.path.basename(label_path_name)\n","    crop_code = get_plant_type_code(basename)\n","    disease_code = get_plant_disease_code(basename)\n","\n","    label = cfg['dict_label'][crop_code][disease_code]\n","\n","    \"\"\"\n","    # JSON 어노테이션 로드\n","    with open(label_path.numpy().decode(\"utf-8\"), 'r') as json_file:\n","        json_decoded = json.load(json_file)\n","\n","    # 어노테이션에서 라벨 추출\n","\n","    crop_code = f'{int(json_decoded[\"annotations\"][\"crop\"]):02d}'\n","    disease_code = f'{int(json_decoded[\"annotations\"][\"disease\"]):02d}'\n","    if crop_code == '01' or disease_code =='01':\n","      print(image_path)\n","      print(label_path)\n","      label = 0\n","    else:\n","      label = cfg['dict_label'][crop_code][disease_code]\n","    \"\"\"\n","\n","    return augmented_image, label"],"metadata":{"id":"wCMiHGg-qE5Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터셋 생성 함수\n","def create_dataset(image_paths, label_paths, batch_size):\n","    \"\"\"\n","    주어진 이미지 파일 경로와 라벨 파일 경로를 사용하여 데이터셋을 생성합니다.\n","\n","    Args:\n","        image_paths (list): 이미지 파일의 경로를 포함하는 리스트.\n","        label_paths (list): 라벨 파일의 경로를 포함하는 리스트.\n","        batch_size (int): 배치 크기.\n","\n","    Returns:\n","        tf.data.Dataset: 이미지와 라벨에 대한 데이터셋.\n","    \"\"\"\n","\n","    # 이미지 파일 경로와 라벨 파일 경로를 텐서로부터 슬라이스하여 데이터셋 생성\n","    dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_paths))\n","\n","    # 각 이미지와 라벨에 대해 로드 및 전처리를 수행하는 함수를 적용\n","    # tf.py_function은 TensorFlow에서 python 함수를 텐서플로 그래프 안에서 실행할 수 있게 해주는 함수\n","    # func: Python 함수로 변환할 함수입니다. 이 함수는 텐서를 입력으로 받아들이고 텐서를 반환해야 합니다.\n","    # inp: Python 함수에 전달될 입력 텐서나 텐서의 목록입니다.\n","    # Tout: Python 함수의 반환값의 데이터 타입을 지정합니다. 이 값은 텐서 또는 텐서의 목록이 될 수 있습니다.\n","    # name: 연산의 이름을 지정합니다. 선택적 매개변수입니다.\n","    dataset = dataset.map(lambda x, y: tf.py_function(\n","        func=load_image_and_label,\n","        inp=[x, y],\n","        Tout=[tf.float32, tf.int64]\n","    ), num_parallel_calls=tf.data.experimental.AUTOTUNE)  # num_parallel_calls은 전처리할때 병렬처리 수 설정\n","    # tf.data.experimental.AUTOTUNE은 TensorFlow가 데이터를 효율적으로 처리하기 위해 필요한 병렬 처리 수를 자동으로 결정\n","\n","    # 데이터셋 요소의 형태를 정의 (이미지의 크기를 다시한번 resize하는 이유는 보험으로 하는것이다)\n","    dataset = dataset.map(lambda x, y: (tf.ensure_shape(x, (512, 512, 3)), tf.ensure_shape(y, ())))\n","\n","    # 데이터셋을 배치로 묶음\n","    dataset = dataset.batch(batch_size)\n","\n","    # 데이터셋을 사용하기 전에 미리 준비하여 성능을 최적화\n","    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","\n","    return dataset"],"metadata":{"id":"J5EEvnGiRPju","executionInfo":{"status":"ok","timestamp":1717857330911,"user_tz":-540,"elapsed":5,"user":{"displayName":"jonghyeon LEE","userId":"11309282343906360601"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["##### 데이터 정규화\n","- albumentations: 데이터 증강 및 정규화 라이브러리\n","  - Compose : 여러 변환을 하나로 붂는다\n","    - Normalize 변환 : 각 채널을 기준으로 이미지를 정규화 (3,512,512)\n","      - mean과 std 매개변수를 통해 각 채널의 평균과 표준 편차 지정하고 그에 맞춰 픽셀 값 변환\n","      - max_pixel_value는 기존의 픽셀이 가지고있는 최소, 최대 픽셀 값을 명시\n","      - p=1.0은 정규화는 반드시 적용함\n","    - Compose의 p=1.0은 가지고 있는 모든 변환을 적용한다\n","- 해당 정규화 값의 범위\n","  - R 채널: [-2.118, 2.251]\n","  - G 채널: [-2.036, 2.427]\n","  - B 채널: [-1.804, 2.640]"],"metadata":{"id":"PpV1ERzzUQn2"}},{"cell_type":"code","source":["# 이미지 데이터 증강 라이브러리\n","import albumentations\n","import numpy as np\n","\n","test_aug = albumentations.Compose([albumentations.Normalize(mean=[0.485, 0.456, 0.406],\n","                                                             std=[0.229, 0.224, 0.225],\n","                                                             max_pixel_value=255.0,\n","                                                             p=1.0)\n","                                   ],\n","                                   p=1.0)"],"metadata":{"id":"orNftP02qBqM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 실행"],"metadata":{"id":"iZZg1T9nqANL"}},{"cell_type":"code","source":["# 확장자\n","image_extensions = ['.jpg', '.jpeg', '.png']\n","label_extensions = ['.json']\n","\n","# 이미지 및 레이블 경로 리스트 생성\n","train_folder = '/content/CustomDataSet/train'\n","valid_folder = '/content/CustomDataSet/valid'"],"metadata":{"id":"NfMKh3vUUkNh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_files = list_files_in_folder(train_folder, image_extensions)\n","label_files = [f'{path}.json' for path in image_files]\n","\n","valid_image_files = list_files_in_folder(valid_folder, image_extensions)\n","valid_label_files = [f'{path}.json' for path in valid_image_files]\n","\n","print(valid_image_files[0])\n","print(valid_label_files[0])"],"metadata":{"id":"qtRsULs0UbzE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터셋 생성\n","batch_size = 1\n","train_dataset = create_dataset(image_files, label_files, batch_size)\n","valid_dataset = create_dataset(valid_image_files, valid_label_files, batch_size)"],"metadata":{"id":"L_At1ok-U7si"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_dataset)"],"metadata":{"id":"CY4gIWnZU9D8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### (번외) 데이터셋 순회 방법\n","- tensorflow\n","  - for 루프 사용\n","    - for data in dataset:\n","  - iter()와 next() 함수사용\n","    - iter()을 통해 순회하고 다음 배치를 탐색하기위해선 next()를 사용한다.\n","      - 장점: 한 번에 하나의 배치만 가져오기 때문에 메모리 사용자가 적다\n","  - take() 함수 사용\n","    - data = dataset.take(1)  # 첫 번째 데이터 가져오기\n","      - 일정 수의 요소를 쉽게 가져올 수 있다.\n","  - map()과 reduce() 함수등 함수형 프로그래밍 방식\n","\n","* tf.data.experimental.cardinality(dataset_name) : 데이터 셋의 크기 확인\n","  - len()과 달리 크기가 동적으로 변하거나 무한할때도 사용가능"],"metadata":{"id":"Aa2LrmEHBijk"}},{"cell_type":"code","source":["tf.data.experimental.cardinality(train_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sudEwZ64Cm4Q","executionInfo":{"status":"ok","timestamp":1717853405678,"user_tz":-540,"elapsed":399,"user":{"displayName":"jonghyeon LEE","userId":"11309282343906360601"}},"outputId":"7db4a1aa-d6db-461e-9353-8882ce747483"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=int64, numpy=94>"]},"metadata":{},"execution_count":173}]},{"cell_type":"code","source":["for image, label in train_dataset.take(1):\n","  print(image.numpy()[0][2].max())\n","  print(image.numpy()[0][0].min())\n","  print(image.numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0vWErFObzSjc","executionInfo":{"status":"ok","timestamp":1717846566611,"user_tz":-540,"elapsed":330,"user":{"displayName":"jonghyeon LEE","userId":"11309282343906360601"}},"outputId":"02246c62-0eef-41c6-c5c5-7bf9dd0a0c70","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.3676689\n","-1.6217272\n","[[[[ 0.3633959   1.2811276   1.0967845 ]\n","   [ 0.39115673  1.3051313   1.1206815 ]\n","   [ 0.38364795  1.3269979   1.0901634 ]\n","   ...\n","   [-1.3008492  -0.18501548 -1.3007509 ]\n","   [-1.2001075  -0.11156814 -1.2069328 ]\n","   [-1.13992    -0.10802912 -1.1402287 ]]\n","\n","  [[ 0.47567677  1.401984    1.2143635 ]\n","   [ 0.32275808  1.323835    1.1098903 ]\n","   [ 0.38122305  1.359533    1.1392682 ]\n","   ...\n","   [-1.2425848  -0.21298565 -1.2447188 ]\n","   [-1.2207943  -0.16225979 -1.246506  ]\n","   [-1.1875482  -0.16084076 -1.218626  ]]\n","\n","  [[ 0.43582493  1.4306722   1.22061   ]\n","   [ 0.43182802  1.3572932   1.1726116 ]\n","   [ 0.47246587  1.3700987   1.2352479 ]\n","   ...\n","   [-1.2044221  -0.17397103 -1.2058775 ]\n","   [-1.2479364  -0.15827627 -1.2033244 ]\n","   [-1.0731435  -0.03210276 -1.1539644 ]]\n","\n","  ...\n","\n","  [[ 1.6495419   2.4285712   2.4831371 ]\n","   [ 1.5810428   2.4110644   2.4308496 ]\n","   [ 1.5973147   2.4201427   2.4436493 ]\n","   ...\n","   [ 0.9455539   1.0851824   1.6163071 ]\n","   [ 0.7994584   0.8701746   1.174586  ]\n","   [ 0.6702869   0.61994755  0.8317892 ]]\n","\n","  [[ 1.6495419   2.4285712   2.4831371 ]\n","   [ 1.5810428   2.4110644   2.4308496 ]\n","   [ 1.5500042   2.4285712   2.448279  ]\n","   ...\n","   [ 0.9581299   1.0980392   1.6291068 ]\n","   [ 0.9252684   1.0578791   1.3908681 ]\n","   [ 0.5773383   0.5851045   0.8493886 ]]\n","\n","  [[ 1.6152923   2.4285712   2.465708  ]\n","   [ 1.5639181   2.3935573   2.378562  ]\n","   [ 1.5545529   2.4285712   2.403889  ]\n","   ...\n","   [ 0.9376103   1.0529894   1.6365448 ]\n","   [ 0.93563694  1.0630251   1.5359524 ]\n","   [ 0.73398626  0.7789809   1.0567348 ]]]]\n"]}]}]}